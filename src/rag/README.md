# RAG Module - Retrieval Augmented Generation

This module contains the core Retrieval Augmented Generation (RAG) capabilities that power the Ollama Personal Assistant. It enables the AI to access and leverage personalized information from the `personal_info.md` file to provide contextually relevant responses to user queries.

## Table of Contents

- [Overview](#overview)
- [Core Components](#core-components)
  - [RAGAssistant Class](#ragassistant-class)
  - [Interactive CLI](#interactive-cli)
- [Implementation Details](#implementation-details)
  - [Initialization Process](#initialization-process)
  - [Query Processing Flow](#query-processing-flow)
  - [Context Retrieval and Augmentation](#context-retrieval-and-augmentation)
  - [Response Generation](#response-generation)
- [Usage Examples](#usage-examples)
- [Configuration Options](#configuration-options)
- [Appendix: Understanding RAG Technology](#appendix-understanding-rag-technology)
  - [What is Retrieval Augmented Generation?](#what-is-retrieval-augmented-generation)
  - [Vector Stores](#vector-stores)
  - [Text Chunking](#text-chunking)
  - [Embeddings](#embeddings)
  - [The Context Window](#the-context-window)
  - [Conversation History](#conversation-history)

## Overview

The RAG module acts as the "memory" of your Personal Assistant. While large language models (LLMs) like Llama have impressive capabilities, they lack personal knowledge about an individual user. The RAG system bridges this gap by:

1. Converting your personal information into a searchable format
2. Finding relevant information when you ask a question
3. Providing this information to the LLM to generate accurate, personalized responses

> [!IMPORTANT]
> The RAG module depends on the `personal_info.md` file generated by the `generation` module. This file is the central knowledge repository that the RAG system searches to provide personalized responses.

## Core Components

### RAGAssistant Class

The `RAGAssistant` class in `assistant.py` is the engine that powers the RAG capabilities:

```
RAGAssistant
├── Initialize vector store from personal_info.md
├── Process user queries
│   ├── Query expansion for better retrieval
│   ├── Context retrieval from vector store
│   └── LLM response generation with retrieved context
├── Conversation history management
└── Vector store management (reloading, model switching)
```

Key methods:
- `__init__`: Sets up the RAG assistant with a vector store from the markdown file
- `get_answer`: Retrieves relevant context and generates a response
- `reload_vector_store`: Updates the vector store when the markdown file changes
- `clear_history`: Manages conversation history

### Interactive CLI

The `interactive.py` file provides a command-line interface for interacting with the RAG assistant. It offers the same functionality as the API endpoint but with an interactive interface that includes:

- Session management
- Parameter adjustment
- Context visualization
- Command-based controls

The CLI interface is designed to help you test and debug your RAG setup before deploying it through the API.

## Implementation Details

### Initialization Process

When a `RAGAssistant` is created, the following happens:

1. **Connection Verification**: The system checks if the Ollama server is reachable
2. **Markdown Processing**: The `personal_info.md` file is loaded and preprocessed
3. **Text Chunking**: The document is split into manageable chunks using `RecursiveCharacterTextSplitter`
4. **Vector Store Creation**: Chunks are embedded and stored in a FAISS vector database

This process creates a searchable knowledge base from your personal information.

### Query Processing Flow

When you ask a question, the RAG system:

1. **Expands your query**: Creates variations to improve retrieval chances
2. **Retrieves context**: Finds the most relevant chunks from your personal information
3. **Builds a prompt**: Combines the retrieved context with your query
4. **Generates a response**: Sends the enhanced prompt to the Ollama model

The system handles all this behind the scenes, providing you with a seamless experience.

### Context Retrieval and Augmentation

The RAG system uses semantic search to find information relevant to your query:

```
Your query → Query expansion → Vector search → 
  Most relevant chunks → Context-enhanced prompt → Response
```

The `_expand_query` method creates variations of your question to catch different phrasings of the same information. For example, if you ask "Who is my sister?", it might also search for "siblings family relationships".

### Response Generation

The final step sends a carefully crafted prompt to the Ollama model:

1. The prompt includes retrieved context chunks
2. Previous conversation history is included for continuity
3. The system instructs the model to respond as your personal assistant
4. The model generates a response using the enhanced context

## Usage Examples

### Basic Usage

```python
from rag.assistant import RAGAssistant

# Initialize the RAG assistant
assistant = RAGAssistant()

# Get an answer to a question
answer, context = assistant.get_answer("What's my wife's name?")
print(answer)  # "Your wife's name is Jane Doe."
```

### Interactive CLI

To start an interactive session:

```bash
python src/main.py interactive --model llama3 --verbose
```

The `--verbose` flag shows the retrieved context for each response.

## Configuration Options

The RAG system can be configured through `config/rag_config.py`:

| Parameter | Description | Default |
|-----------|-------------|---------|
| OLLAMA_API_HOST | URL of the Ollama API server | http://192.168.1.124:11434 |
| OLLAMA_DEFAULT_MODEL | Default model for embeddings and generation | oliver-assistant |
| DEFAULT_CHUNK_SIZE | Size of text chunks for the vector store | 1000 |
| DEFAULT_CHUNK_OVERLAP | Overlap between text chunks | 200 |
| DEFAULT_TOP_K | Number of chunks to retrieve | 5 |

## Appendix: Understanding RAG Technology

### What is Retrieval Augmented Generation?

Retrieval Augmented Generation (RAG) is a technique that enhances large language models by providing them with relevant external information at query time. Traditional LLMs can only "know" what they learned during training, but RAG systems can access up-to-date or personalized information.

The process works in two main stages:

1. **Retrieval**: Finding relevant information from a knowledge source
2. **Augmentation**: Using that information to enhance the LLM's response

This approach has several advantages:
- Provides access to information not in the model's training data
- Keeps responses grounded in factual information
- Enables personalization without retraining the model
- Reduces hallucinations (made-up information)

### Vector Stores

Vector stores are specialized databases designed to store and efficiently search vector embeddings. In our implementation:

- We use FAISS (Facebook AI Similarity Search), a library for efficient similarity search
- Each chunk of text from your `personal_info.md` is converted to a vector embedding
- These vectors capture the semantic meaning of the text
- When you ask a question, it's converted to a vector in the same space
- The system finds chunks with vectors most similar to your question's vector

This allows the system to find semantically relevant information even if the exact words don't match.

```
Text chunk → Embedding → Vector (1.2, 0.8, -0.6, ...) → Stored in FAISS
Query      → Embedding → Vector (1.1, 0.7, -0.5, ...) → Compared to stored vectors
```

### Text Chunking

Text chunking is the process of breaking down large documents into smaller, manageable pieces for efficient retrieval. This module uses:

- **RecursiveCharacterTextSplitter**: A sophisticated chunking algorithm that:
  - Splits text at natural boundaries (headers, paragraphs, etc.)
  - Respects markdown structure for more coherent chunks
  - Creates chunks of approximately uniform size
  - Maintains overlap between chunks to preserve context

The chunking process is configured with:
- `chunk_size`: Target size of each text chunk (1000 characters by default)
- `chunk_overlap`: Number of characters shared between adjacent chunks (200 characters by default)
- `MARKDOWN_SEPARATORS`: List of strings that indicate natural splitting points

Proper chunking is crucial for effective retrieval - chunks should be large enough to contain complete information but small enough to be specific.

### Embeddings

Embeddings are numerical representations of text that capture semantic meaning. This module uses:

- **OllamaEmbeddings**: A class that interfaces with the Ollama API to generate embeddings
- The same model used for response generation is used for creating embeddings
- Each chunk of text and each query is converted to a high-dimensional vector
- Similarity between these vectors indicates semantic relevance

The embedding process transforms text from a sequence of tokens to a fixed-length vector:
```
"What is my wife's name?" → [0.021, -0.412, 0.599, ..., 0.132]
```

Semantically similar questions will have similar vector representations, allowing the system to find relevant information even when the phrasing differs.

### The Context Window

The context window is the amount of text a language model can "see" at once when generating a response. This is a fundamental limitation of transformer-based models:

- Modern LLMs have context windows ranging from 4K to 128K tokens
- Each token is roughly 3/4 of a word
- The context window must contain:
  - System instructions
  - Retrieved context information
  - The user's question
  - Conversation history
  - Space for the generated response

The RAG system manages the context window by:
- Retrieving only the most relevant chunks (controlled by `top_k`)
- Carefully formatting the prompt to maximize the use of available tokens
- Limiting the conversation history to recent exchanges

### Conversation History

The conversation history stored in the `RAGAssistant` class:

1. Maintains continuity across multiple queries
2. Allows the model to refer back to previous exchanges
3. Provides context for ambiguous questions
4. Enables the assistant to build upon previous answers

For example, if you ask "When is her birthday?" after asking about your wife, the system can use the conversation history to understand that "her" refers to your wife.

The implementation in this module:
- Stores conversation history as (query, response) tuples
- Limits history to the 10 most recent exchanges
- Formats history in the prompt to maintain conversational context
- Can be cleared manually if needed

> [!TIP]
> The conversation history is automatically managed, but you can use the `/clear_history` command in the interactive CLI or call the `clear_history()` method to reset it when starting a new topic.